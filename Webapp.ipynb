{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"TPU","colab":{"name":"Webapp.ipynb","provenance":[{"file_id":"1GjJHUcDRvYS-goKXZjNboS9mJw25P28q","timestamp":1607085019069}],"collapsed_sections":[],"authorship_tag":"ABX9TyMh7MZG01U63KEhTtPHetMX"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"id":"104ciPwrR-8F","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607088572361,"user_tz":-660,"elapsed":21569,"user":{"displayName":"Sen Yang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi-lw8VOax0w3XdcteQSSdaCY7I-ASY7iZyGjoy=s64","userId":"12751544315343024734"}},"outputId":"7118780e-385b-4c35-e04c-4c5dee466749"},"source":["!pip install streamlit -q"],"execution_count":11,"outputs":[{"output_type":"stream","text":["\u001b[K     |████████████████████████████████| 7.4MB 6.5MB/s \n","\u001b[K     |████████████████████████████████| 102kB 9.0MB/s \n","\u001b[K     |████████████████████████████████| 163kB 43.1MB/s \n","\u001b[K     |████████████████████████████████| 112kB 54.8MB/s \n","\u001b[K     |████████████████████████████████| 4.5MB 33.6MB/s \n","\u001b[K     |████████████████████████████████| 71kB 8.5MB/s \n","\u001b[K     |████████████████████████████████| 122kB 35.6MB/s \n","\u001b[?25h  Building wheel for watchdog (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for blinker (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[31mERROR: google-colab 1.0.0 has requirement ipykernel~=4.10, but you'll have ipykernel 5.3.4 which is incompatible.\u001b[0m\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"K_WQBRhxSBOk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607088478124,"user_tz":-660,"elapsed":1812,"user":{"displayName":"Sen Yang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi-lw8VOax0w3XdcteQSSdaCY7I-ASY7iZyGjoy=s64","userId":"12751544315343024734"}},"outputId":"a7e7b7bd-19f2-43a4-ec5e-258bc5f3a668"},"source":["!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n","!unzip -qq ngrok-stable-linux-amd64.zip"],"execution_count":2,"outputs":[{"output_type":"stream","text":["--2020-12-04 13:27:58--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n","Resolving bin.equinox.io (bin.equinox.io)... 54.225.42.45, 3.230.235.205, 52.1.26.21, ...\n","Connecting to bin.equinox.io (bin.equinox.io)|54.225.42.45|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 13773305 (13M) [application/octet-stream]\n","Saving to: ‘ngrok-stable-linux-amd64.zip’\n","\n","ngrok-stable-linux- 100%[===================>]  13.13M  36.0MB/s    in 0.4s    \n","\n","2020-12-04 13:27:58 (36.0 MB/s) - ‘ngrok-stable-linux-amd64.zip’ saved [13773305/13773305]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"H6IxkIkdUKXQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607088489884,"user_tz":-660,"elapsed":10407,"user":{"displayName":"Sen Yang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi-lw8VOax0w3XdcteQSSdaCY7I-ASY7iZyGjoy=s64","userId":"12751544315343024734"}},"outputId":"a1aa4b67-c654-4256-c3ce-679280da2861"},"source":["! pip install pytorch_pretrained_bert"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Collecting pytorch_pretrained_bert\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n","\u001b[K     |████████████████████████████████| 133kB 3.3MB/s \n","\u001b[?25hRequirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.7.0+cu101)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.18.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2.23.0)\n","Collecting boto3\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5c/8f/93e490e99c25bebbc79f762448866acf4b942ea6666423ac308c2aedb800/boto3-1.16.29-py2.py3-none-any.whl (129kB)\n","\u001b[K     |████████████████████████████████| 133kB 10.4MB/s \n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (4.41.1)\n","Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2019.12.20)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (0.16.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (3.7.4.3)\n","Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (0.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2020.11.8)\n","Collecting s3transfer<0.4.0,>=0.3.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/79/e6afb3d8b0b4e96cefbdc690f741d7dd24547ff1f94240c997a26fa908d3/s3transfer-0.3.3-py2.py3-none-any.whl (69kB)\n","\u001b[K     |████████████████████████████████| 71kB 5.2MB/s \n","\u001b[?25hCollecting botocore<1.20.0,>=1.19.29\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0c/90/bb838125929b0226ea672f5ba80469aabc915ec10b3f9a8e75ede1f062a7/botocore-1.19.29-py2.py3-none-any.whl (7.0MB)\n","\u001b[K     |████████████████████████████████| 7.0MB 16.2MB/s \n","\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n","  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.20.0,>=1.19.29->boto3->pytorch_pretrained_bert) (2.8.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.20.0,>=1.19.29->boto3->pytorch_pretrained_bert) (1.15.0)\n","\u001b[31mERROR: botocore 1.19.29 has requirement urllib3<1.27,>=1.25.4; python_version != \"3.4\", but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n","Installing collected packages: jmespath, botocore, s3transfer, boto3, pytorch-pretrained-bert\n","Successfully installed boto3-1.16.29 botocore-1.19.29 jmespath-0.10.0 pytorch-pretrained-bert-0.6.2 s3transfer-0.3.3\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"R8WWg0eQUNtK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607088499228,"user_tz":-660,"elapsed":8655,"user":{"displayName":"Sen Yang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi-lw8VOax0w3XdcteQSSdaCY7I-ASY7iZyGjoy=s64","userId":"12751544315343024734"}},"outputId":"75318de4-de32-4995-ab9e-4faf9de45480"},"source":["! pip install transformers==3.5.1"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Collecting transformers==3.5.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/83/e74092e7f24a08d751aa59b37a9fc572b2e4af3918cb66f7766c3affb1b4/transformers-3.5.1-py3-none-any.whl (1.3MB)\n","\u001b[K     |████████████████████████████████| 1.3MB 5.1MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.1) (3.0.12)\n","Collecting sentencepiece==0.1.91\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n","\u001b[K     |████████████████████████████████| 1.1MB 25.3MB/s \n","\u001b[?25hCollecting tokenizers==0.9.3\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/34/b39eb9994bc3c999270b69c9eea40ecc6f0e97991dba28282b9fd32d44ee/tokenizers-0.9.3-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n","\u001b[K     |████████████████████████████████| 2.9MB 35.3MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.1) (1.18.5)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 36.6MB/s \n","\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.1) (3.12.4)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.1) (20.4)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.1) (4.41.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.1) (2.23.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.1) (2019.12.20)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.1) (0.8)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.5.1) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.5.1) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.5.1) (0.17.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers==3.5.1) (50.3.2)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==3.5.1) (2.4.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.5.1) (2020.11.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.5.1) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.5.1) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.5.1) (1.24.3)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=da48bc389ef6d60a1da57bfabfc622530a5a3bf38257697e434a485a5384ee5e\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built sacremoses\n","Installing collected packages: sentencepiece, tokenizers, sacremoses, transformers\n","Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.9.3 transformers-3.5.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h-OhQVbHUaMG","executionInfo":{"status":"ok","timestamp":1607088532678,"user_tz":-660,"elapsed":33437,"user":{"displayName":"Sen Yang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi-lw8VOax0w3XdcteQSSdaCY7I-ASY7iZyGjoy=s64","userId":"12751544315343024734"}},"outputId":"b9824e61-372c-4ba6-b12b-c8360f5a1c65"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E2L-XgA8Sm1Z","executionInfo":{"status":"ok","timestamp":1607088537001,"user_tz":-660,"elapsed":885,"user":{"displayName":"Sen Yang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi-lw8VOax0w3XdcteQSSdaCY7I-ASY7iZyGjoy=s64","userId":"12751544315343024734"}},"outputId":"e8cbbca4-c085-40ee-be0d-c6aa67548d4a"},"source":["%%writefile web.py\n","\n","import pytorch_pretrained_bert\n","import streamlit as st\n","import numpy as np\n","import pandas as pd\n","import altair as alt\n","from pip._vendor.urllib3.packages.six import StringIO\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","import pandas as pd\n","from transformers import AutoTokenizer, AutoModelWithLMHead\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n","from tqdm import tqdm\n","import time\n","from datetime import timedelta\n","from importlib import import_module\n","import torch\n","import numpy as np\n","import torch.nn as nn\n","from transformers import BertModel, BertTokenizer\n","from transformers import AutoTokenizer, AutoModelWithLMHead, AdamW\n","import torch.nn.functional as F\n","from sklearn import metrics\n","from pytorch_pretrained_bert import *\n","import transformers\n","\n","import matplotlib.pyplot as plt\n","import spacy   # for tokenising text\n","from spacy.lang.en import English  # for tokenising text\n","nlp = English()   # for tokenising text\n","from collections import Counter   # for getting freq of words\n","from spacy.matcher import PhraseMatcher\n","import seaborn as sns   # for charts\n","sns.set_style(\"whitegrid\");   # chart background style\n","plt.rcParams['figure.dpi'] = 360   # for high res chart output\n","import re   # for regular expressions\n","\n","PAD, CLS = '[PAD]', '[CLS]'\n","\n","\n","def main():\n","    st.title(\"Toxic Word Detection API\")\n","\n","    activities = ['Detection', 'Dataset Display', 'Dataset Analysis 1', 'Dataset Analysis 2']\n","    choice = st.sidebar.selectbox(\"Select Choice\", activities)\n","\n","    if choice == 'Detection':\n","        st.subheader(\"Toxic Word Detection\")\n","        user_input = acp_use_input()\n","        st.write('The text is', user_input)\n","\n","        if len(user_input) == 0:\n","            st.info(\"Please enter the text you want to classify!\")\n","            # st.text(\"Please enter the text: \")\n","            \n","        else:\n","\n","            print(\"input is \", user_input)\n","\n","            test_data = load_dataset(user_input, config.pad_size)\n","            print(\"test data is \", test_data)\n","            test_iter = build_iterator(test_data, config)\n","            print('datatiter is ', test_iter.batch_size)\n","            print('datatiter is ', test_iter.batches)\n","            print('datatiter is ', test_iter.device)\n","\n","            model = Model(config).to(config.device)\n","            model.load_state_dict(torch.load(config.save_path, map_location=torch.device('cpu')),  strict=False)\n","            model.eval()\n","\n","            predict_label = evaluate(config, model, test_iter, test=True)\n","\n","            if predict_label == 1:\n","              st.write(\"The predicted label for entered text is Toxic!\")\n","            else:\n","              st.write(\"The predicted label for entered text is Non-toxic!\")           \n","\n","            # outputs = BERT_Predict(model, user_input)\n","\n","    elif choice == \"Dataset Display\":\n","        st.subheader(\"Dataset Display\")\n","        path_chat_data = '/content/gdrive/My Drive/materials/labeled_Chat.csv'\n","        df = load_data(path_chat_data)\n","        # df.columns = ['match_id', 'key', 'slot', 'time', 'unit', 'label']\n","        df.rename(columns = {'0': 'label'}, inplace=True)\n","        df.loc[1, 'label'] = 0\n","        # df = df.head(20)\n","        # st.dataframe(df, 1500, 1200)\n","\n","        contents_filter = st.slider(\"Choose only to show toxic chat or non-toxic chat\", 0, 2, 2, 1)\n","        st.info(\"Select 0 will show non-toxic chat; Select 1 will show toxic chat; Select 2 will show all data\")\n","        if contents_filter == 2:\n","          df = df.head(20)\n","          st.dataframe(df, 1500, 1200)\n","        elif contents_filter == 1:\n","          df_filted = df[df['label'] == 1]\n","          st.dataframe(df_filted, 1500, 1200)          \n","        else:         \n","          df_filted = df[df['label'] == 0]\n","          df_filted = df_filted.head(20)\n","          st.dataframe(df_filted, 1500, 1200)\n","\n","    # new feature WEEK9\n","    elif choice == 'Dataset Analysis 1':\n","        st.set_option('deprecation.showPyplotGlobalUse', False)\n","\n","        if st.checkbox('1. Communication frequency of all matches', 1):\n","            # st.subheader('1. Communication frequency of all matches')\n","            path_chat_data = '/content/gdrive/My Drive/materials/chat.csv'\n","            df_chat_data = load_data(path_chat_data)\n","\n","            time = df_chat_data['time']\n","            # fig1 = plt.subplots()\n","            plt.figure(1)\n","            plt.hist(time, bins=100, facecolor=\"purple\")\n","            plt.ylabel(\"Count of chat\")\n","            plt.xlabel(\"Time series(second)\")\n","            plt.title(\"Communication frequency of all matches\")\n","            st.pyplot()\n","\n","            with st.beta_expander(\"Check analysis\"):\n","                st.write(\"\"\"\n","                    This figure indicates that the communication in the game mainly concentrated on around 2500 sec.\n","                \"\"\")\n","            # st.set_option('deprecation.showPyplotGlobalUse', False)\n","\n","        # if st.checkbox('2. Player win rate distribution', 2):\n","        #     # st.subheader('')\n","        #     # draw_figure2()\n","        #     path_data = '/content/gdrive/My Drive/materials/player_ratings.csv'\n","        #     df_data = load_data(path_data)\n","        #     total_wins = df_data[\"total_wins\"].values\n","        #     total_matches = df_data[\"total_matches\"].values\n","\n","\n","        #     total_wins, total_matches = (list(t) for t in zip(*sorted(zip(total_wins, total_matches))))\n","        #     plt.figure(2)\n","        #     plt.plot(total_wins[0:-1], total_matches[0:-1])\n","        #     plt.xlabel('total matches')\n","        #     plt.ylabel('total wins')\n","\n","        #     st.pyplot()\n","\n","        #     with st.beta_expander(\"Check analysis\"):\n","        #         st.write(\"\"\"\n","        #             This figure indicates that the win rate of most players is approximately 50%.\n","        #         \"\"\")\n","\n","        if st.checkbox('2. Player win rate distribution', 2):\n","            path_data = '/content/gdrive/My Drive/materials/player_ratings.csv'\n","            df_data = load_data(path_data)\n","            total_wins = df_data[\"total_wins\"].values\n","            total_matches = df_data[\"total_matches\"].values\n","\n","\n","            total_wins, total_matches = (list(t) for t in zip(*sorted(zip(total_wins, total_matches))))\n","            y=total_wins[0:-1]\n","            x=total_matches[0:-1]        \n","\n","            plt.figure(2)\n","\n","            plt.scatter(x,y)\n","            z= np.polyfit(x, y, 1)\n","            p = np.poly1d(z)\n","\n","            plt.plot(x,p(x),\"-\",color='#FFA500')\n","            plt.xlabel('total matches ')\n","            plt.ylabel('total wins')\n","            plt.title('Player win rates distribution')\n","            st.pyplot()\n","\n","            with st.beta_expander(\"Check analysis\"):\n","                st.write(\"\"\"\n","                    This figure indicates that the player's true skill will tend to be stable while the total matches increases.\n","                \"\"\")\n","\n","        if st.checkbox('3. Player skills level distribution (mu)', 3):\n","            # st.subheader('')\n","            path_data = '/content/gdrive/My Drive/materials/player_ratings.csv'\n","            df_data = load_data(path_data)\n","            trueskill_mu1 = df_data[\"trueskill_mu\"].values\n","\n","            plt.figure(3)\n","\n","            plt.hist(trueskill_mu1, bins=40, histtype='bar', edgecolor=\"black\", align=\"mid\")\n","            plt.xlabel('trueskill mu')\n","            plt.ylabel(\"Players\")\n","            st.pyplot()\n","\n","            with st.beta_expander(\"Check analysis\"):\n","                st.write(\"\"\"\n","                    This figure indicates that overall skill of players follows the mu curve.\n","                \"\"\")\n","\n","\n","        if st.checkbox('4. Relation between kills and toxic event', 4):\n","            # st.subheader('')\n","            path_data_1 = '/content/gdrive/My Drive/materials/match_teamfight_count_mean_chatnum_toxic.csv'\n","            df_data_1 = load_data(path_data_1)\n","            # df_data_1[['first_blood_time', 'toxic_words_count']]\n","            df_data_1[['first_blood_time', 'toxic_words_count']].sort_values(\"first_blood_time\", inplace=False)\n","            path_data_2 = '/content/gdrive/My Drive/materials/players.csv'\n","            df_data_2 = load_data(path_data_2)\n","            result = pd.merge(df_data_1, df_data_2, how='outer', on=['match_id'])\n","            listbin2s = [i * 5 for i in range(13)]\n","            result['kills'] = pd.cut(result['kills'], listbin2s, right=True, labels=None, retbins=False, precision=3,\n","                                     include_lowest=False)\n","            thirdy = list(\n","                result[['kills', 'toxic_words_count']].sort_values(\"kills\", inplace=False).groupby('kills').mean()[\n","                    'toxic_words_count'])\n","\n","            plt.figure(4)\n","            l1 = plt.plot(listbin2s[1:], thirdy)\n","\n","            plt.title('Relation between kills and toxic event')\n","            plt.xlabel('kills')\n","            plt.ylabel('toxic_word_count')\n","\n","            st.pyplot()\n","\n","            with st.beta_expander(\"Check analysis\"):\n","                st.write(\"\"\"\n","                    This figure indicates that the relation between toxic word and kill event.\n","                \"\"\")\n","\n","        if st.checkbox('5. Player skills level distribution (sigma)', 5):\n","            path_data = '/content/gdrive/My Drive/materials/player_ratings.csv'\n","            df_data = load_data(path_data)\n","            trueskill_sigma = df_data[\"trueskill_sigma\"].values\n","            total_matches = df_data[\"total_matches\"].values\n","\n","            plt.figure(5)\n","\n","            plt.scatter(total_matches[0:-1],trueskill_sigma[0:-1])\n","            plt.xlabel('total matches')\n","            plt.ylabel('trueskill sigma')\n","            st.pyplot()\n","\n","            with st.beta_expander(\"Check analysis\"):\n","                st.write(\"\"\"\n","                    This figure indicates that the player's true skill will tend to be stable while the total matches increases.\n","                \"\"\")\n","\n","    elif choice == 'Dataset Analysis 2':\n","        if st.checkbox('1. Top 30 Most Frequent Bad Words in the Game', 1):\n","            # st.subheader('')\n","            path_data = '/content/gdrive/My Drive/materials/Output.csv'\n","            chat_df= load_data(path_data)\n","            # chat_df.columns = ['match_id', 'key', 'slot', 'time', 'unit']\n","            chat_string = ' '.join(chat for chat in chat_df['key'])\n","            # lower case and remove extra white spaces\n","            chat_string = chat_string.lower()\n","            chat_string = re.sub(r'\\s+', ' ', chat_string)\n","\n","            # create spacy document\n","            len(chat_string)\n","            nlp.max_length = 16700000\n","            nlp.Defaults.stop_words.add('u')\n","            chat_doc = nlp(chat_string)\n","\n","            toxic_words_lexicon = load_lexcion()\n","\n","            # match lexicon with document\n","            matcher = PhraseMatcher(nlp.vocab)\n","            terms_list = toxic_words_lexicon\n","            patterns = [nlp.make_doc(text) for text in terms_list]\n","            matcher.add('phrase_matcher', None, *patterns)\n","            character_matches = matcher(chat_doc)\n","\n","            c = []\n","            for match_id, start, end in character_matches:\n","                rule_id = nlp.vocab.strings[match_id]\n","                span = chat_doc[start:end]\n","                c.append(span.text)\n","            # print(c)\n","\n","            # Calculate top 30 bad words frequency\n","            batwords_word_freq = Counter(c)\n","            top30badwords = batwords_word_freq.most_common(30)\n","\n","            freq_df = pd.DataFrame.from_dict(batwords_word_freq, orient='index').reset_index()\n","            freq_df.columns = [\"word\", \"freq\"]\n","            freq_df.sort_values(by=\"freq\", ascending=False).head(30)\n","\n","\n","            plt.figure(1)\n","            fig, ax = plt.subplots(figsize=(20, 10))\n","            sns.barplot(data=freq_df.sort_values(by=\"freq\", ascending=False).head(30),\n","                        y=\"word\",\n","                        x=\"freq\",\n","                        color='#7bbcd5')\n","            plt.ylabel(\"Word\")\n","            plt.xlabel(\"Frequency\")\n","            plt.title(\"Top 30 Most Frequent Bad Words in the Game\")\n","            sns.despine()\n","\n","            st.pyplot()\n","\n","            with st.beta_expander(\"Check analysis\"):\n","                st.write(\"\"\"\n","                    This figure indicates that Top 30 Most Frequent Bad Words in the Game.\n","                \"\"\")\n","\n","        if st.checkbox('2. First blood time in the game', 2):\n","\n","            path_data = '/content/gdrive/My Drive/materials/match.csv'\n","            df_data= load_data(path_data)\n","\n","            first_blood_temp = df_data['first_blood_time']\n","            first_blood = []\n","            for i in first_blood_temp:\n","                if i != 0:\n","                    first_blood.append(i)\n","            first_blood_mean = sum(first_blood) / len(first_blood)\n","            first_blood_max = max(first_blood)\n","            first_blood_min = min(first_blood)\n","\n","            plt.figure(2)\n","\n","            plt.boxplot(first_blood, showfliers=False)\n","            plt.ylabel('Time')\n","            plt.xticks(labels=['First Blood'], ticks=[1])\n","\n","            st.pyplot()\n","\n","            with st.beta_expander(\"Check analysis\"):\n","                st.write(\"\"\"\n","                    This figure indicates that the first blood time in the game.\n","                \"\"\")\n","\n","\n","        if st.checkbox('3. Relation between kills and toxic event', 3):\n","            # st.subheader('')\n","            path_data_1 = '/content/gdrive/My Drive/materials/match_teamfight_count_mean_chatnum_toxic.csv'\n","            df_data_1 = load_data(path_data_1)\n","            # df_data_1[['first_blood_time', 'toxic_words_count']]\n","            df_data_1[['first_blood_time', 'toxic_words_count']].sort_values(\"first_blood_time\", inplace=False)\n","            path_data_2 = '/content/gdrive/My Drive/materials/players.csv'\n","            df_data_2 = load_data(path_data_2)\n","            result = pd.merge(df_data_1, df_data_2, how='outer', on=['match_id'])\n","\n","            listbins = [i*5 for i in range(10)]\n","            listbin2s = [i * 5 for i in range(13)]\n","          \n","            result['deaths'] = pd.cut(result['deaths'],listbins,right=True,labels=None,retbins=False,precision=3,include_lowest=False)\n","            result['kills'] = pd.cut(result['kills'],listbin2s,right=True,labels=None,retbins=False,precision=3,include_lowest=False)\n","\n","            resulty = list(result[['deaths','toxic_words_count']].sort_values(\"deaths\",inplace=False).groupby('deaths').mean()['toxic_words_count'])\n","            thirdy = list(result[['kills','toxic_words_count']].sort_values(\"kills\",inplace=False).groupby('kills').mean()['toxic_words_count'])\n","\n","            plt.figure(3)\n","            l1=plt.plot(listbins[1:],resulty)\n","            l2=plt.plot(listbin2s[1:10],thirdy[:9],label = 'kills')\n","\n","            plt.title('Relation between deaths/kills and toxic event')\n","            plt.xlabel('deaths/kills')\n","            plt.ylabel('toxic_word_count')\n","            st.pyplot()\n","\n","            with st.beta_expander(\"Check analysis\"):\n","                st.write(\"\"\"\n","                    This figure indicates the relation between deaths/kills and toxic event.\n","                \"\"\")\n","\n","\n","        if st.checkbox('4. Teamfights start points', 4):\n","\n","\n","            x,y = load_teamfights()\n","\n","            plt.figure(4)\n","            plt.plot(x,y)\n","            plt.ylabel(\"Count of start points\")\n","            plt.xlabel(\"Time series(second)\")\n","            plt.title(\"Scatter plots of teamfights start points\")\n","            st.pyplot()\n","\n","            with st.beta_expander(\"Check analysis\"):\n","                st.write(\"\"\"\n","                    This figure indicates start points of teamfights.\n","                \"\"\")\n","\n","        if st.checkbox('5. Teamfights end points', 5):\n","\n","\n","            x1,y1 = load_teamfights_end()\n","\n","            plt.figure(5)\n","            plt.plot(x1,y1)\n","            plt.ylabel(\"Count of end points\")\n","            plt.xlabel(\"Time series(second)\")\n","            plt.title(\"Scatter plots of teamfights end points\")\n","            st.pyplot()\n","\n","            with st.beta_expander(\"Check analysis\"):\n","                st.write(\"\"\"\n","                    This figure indicates end points of teamfights.\n","                \"\"\")\n","    # st.write(user_input.dtypes)\n","\n","@st.cache\n","def load_teamfights_end():\n","    path_data = '/content/gdrive/My Drive/materials/teamfights.csv'\n","    df_data = load_data(path_data)\n","\n","    teamfight_end = df_data['end']\n","    i1=100\n","    x1=[]\n","    y1=[]\n","    while i1<16000:\n","      x1.append(i1);\n","      temp1=0\n","      for a in teamfight_end:\n","        if a <=i1 and a >=i1-10:\n","          temp1+=1\n","      y1.append(temp1)\n","      i1+=100\n","\n","    return x1, y1\n","\n","\n","@st.cache\n","def load_teamfights():\n","    path_data = '/content/gdrive/My Drive/materials/teamfights.csv'\n","    df_data = load_data(path_data)\n","\n","    teamfight_start = df_data['start']\n","    i1=100\n","    x=[]\n","    y=[]\n","    while i1<16000:\n","      x.append(i1);\n","      temp1=0\n","      for a in teamfight_start:\n","        if a <=i1 and a >=i1-10:\n","          temp1+=1\n","      y.append(temp1)\n","      i1+=100\n","    \n","    return x,y \n","\n","\n","@st.cache\n","def load_lexcion():\n","    toxic_words = []\n","\n","    # 需要自己重新上传list_google_lexcon.txt\n","    for line in open(\"/content/gdrive/My Drive/materials/list_google_lexicon.txt\"):\n","        toxic_words.append(line[:-1])\n","\n","    for line in open(\"/content/gdrive/My Drive/materials/profanity_wordlist.txt\"):\n","        toxic_words.append(line[:-1])\n","\n","    # 这部分可以自己手动添加新的list 扩充lexicon\n","    toxic_words.extend(\n","        ['noob', 'noobs', 'retard', 'retards', 'tard', 'tards', 'idiot', 'idiots', 'stupid', 'moron', 'morons', 'fu',\n","         'nap'])\n","\n","    toxic_words_new = []\n","    for i in toxic_words:\n","        if i not in toxic_words_new:\n","            toxic_words_new.append(i)\n","\n","    return toxic_words_new\n","\n","@st.cache\n","def load_data(path):\n","    df = pd.read_csv(path, header=0)\n","    return df\n","\n","\n","class Config(object):\n","    \"\"\"配置参数\"\"\"\n","\n","    def __init__(self):\n","        self.model_name = 'bert'  # 测试集\n","        self.class_list = [x.strip() for x in open(\n","            '/content/gdrive/My Drive/materials/labels_phrase_opi.txt').readlines()]  # 类别名单\n","        self.save_path = '/content/gdrive/My Drive/materials/saved_dict_opi_ordered_chen_new'\n","        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # 设备\n","\n","        # self.require_improvement = 1000  # 若超过1000batch效果还没提升，则提前结束训练\n","\n","        self.require_improvement = 100000 \n","\n","        self.num_classes = len(self.class_list)\n","        # self.num_classes = 2                         # 类别数\n","        self.num_epochs = 5  # epoch数\n","\n","        # self.batch_size = 64           \n","        self.batch_size = 1  # mini-batch大小\n","\n","        self.pad_size = 32  # 每句话处理成的长度(短填长切)\n","        self.learning_rate = 5e-5\n","        # 改成下载的预训练模型路径\n","        # self.bert_path = ('/content/gdrive/My Drive/materials/bert-base-uncased')\n","\n","        self.bert_model_choose = \"bert-base-uncased\"\n","        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","        # self.bert_model_choose = \"bert-base-multilingual-uncased\"\n","        # self.tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-uncased\")\n","\n","        self.hidden_size = 768\n","\n","\n","config = Config()\n","\n","\n","class DatasetIterater(object):\n","    def __init__(self, batches, batch_size, device):\n","        self.batch_size = batch_size\n","        self.batches = batches\n","        self.n_batches = len(batches) // batch_size\n","        self.residue = False  # 记录batch数量是否为整数\n","        if len(batches) % self.n_batches != 0:\n","            self.residue = True\n","        self.index = 0\n","        self.device = device\n","\n","    def _to_tensor(self, datas):\n","        x = torch.LongTensor([_[0] for _ in datas]).to(self.device)\n","        y = torch.LongTensor([_[1] for _ in datas]).to(self.device)\n","\n","        # pad前的长度(超过pad_size的设为pad_size)\n","        seq_len = torch.LongTensor([_[2] for _ in datas]).to(self.device)\n","        mask = torch.LongTensor([_[3] for _ in datas]).to(self.device)\n","\n","\n","        # print(\"x is \", x)\n","        # print(\"y is \", y)\n","        # print(\"seq len is \", seq_len)\n","        # print(\"mask is \", mask)\n","\n","        return (x, seq_len, mask), y\n","\n","    def __next__(self):\n","        if self.residue and self.index == self.n_batches:\n","            batches = self.batches[self.index * self.batch_size: len(self.batches)]\n","            self.index += 1\n","            batches = self._to_tensor(batches)\n","            return batches\n","\n","        elif self.index >= self.n_batches:\n","            self.index = 0\n","            raise StopIteration\n","        else:\n","            batches = self.batches[self.index * self.batch_size: (self.index + 1) * self.batch_size]\n","            self.index += 1\n","            batches = self._to_tensor(batches)\n","            return batches\n","\n","    def __iter__(self):\n","        return self\n","\n","    def __len__(self):\n","        if self.residue:\n","            return self.n_batches + 1\n","        else:\n","            return self.n_batches\n","\n","\n","class Model(nn.Module):\n","\n","    def __init__(self, config):\n","        super(Model, self).__init__()\n","        # self.bert = BertModel.from_pretrained(config.bert_path)\n","        self.bert = BertModel.from_pretrained(config.bert_model_choose)\n","        for param in self.bert.parameters():\n","            param.requires_grad = True\n","        self.fc = nn.Linear(config.hidden_size, config.num_classes)\n","\n","    def forward(self, x):\n","        context = x[0]\n","        print(\"context is \", context)\n","        mask = x[2]\n","        print(\"mask is \", mask)\n","        _, pooled = self.bert(context, attention_mask=mask)\n","        out = self.fc(pooled)\n","\n","        return out\n","\n","\n","def load_dataset(test_data, pad_size=32):\n","    contents = []\n","    token = config.tokenizer.tokenize(test_data)\n","    token = [CLS] + token\n","    seq_len = len(token)\n","    mask = []\n","    token_ids = config.tokenizer.convert_tokens_to_ids(token)\n","    label = \"0\"\n","\n","    if pad_size:\n","        if len(token) < pad_size:\n","            mask = [1] * len(token_ids) + [0] * (pad_size - len(token))\n","            token_ids += ([0] * (pad_size - len(token)))\n","        else:\n","            mask = [1] * pad_size\n","            token_ids = token_ids[:pad_size]\n","            seq_len = pad_size\n","    contents.append((token_ids, int(label), seq_len, mask))\n","    return contents\n","\n","\n","def build_iterator(dataset, config):\n","    iter = DatasetIterater(dataset, config.batch_size, config.device)\n","    return iter\n","\n","\n","\n","def evaluate(config, model, data_iter, test=False):\n","    model.eval()\n","    loss_total = 0\n","    predict_all = np.array([], dtype=int)\n","    labels_all = np.array([], dtype=int)\n","    print('_____')\n","    with torch.no_grad():\n","        for texts, labels in data_iter:\n","            outputs = model(texts)\n","            print(\"ssfsf si\", outputs)\n","            loss = F.cross_entropy(outputs, labels)\n","            loss_total += loss\n","            labels = labels.data.cpu().numpy()\n","            predic = torch.max(outputs.data, 1)[1].cpu().numpy()\n","\n","            print(\"lal is \", predic)\n","\n","            labels_all = np.append(labels_all, labels)\n","            predict_all = np.append(predict_all, predic)\n","\n","            print(\"lal1 is \", predict_all)\n","\n","    return predict_all\n","\n","def acp_use_input():\n","    st.write(\"Enter your text: \")\n","    text_input = st.text_input(\" \", value='', key=None)\n","    # df_input = pd.DataFrame(np.array([text_input]).reshape(1, -1))\n","        #\n","        # return df_input\n","    return text_input\n","\n","\n","if __name__ == '__main__':\n","    main()\n","\n"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Overwriting web.py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"SsO4Vb1c8RDT"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c2-ro80R8X4p","executionInfo":{"status":"ok","timestamp":1607088541683,"user_tz":-660,"elapsed":755,"user":{"displayName":"Sen Yang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi-lw8VOax0w3XdcteQSSdaCY7I-ASY7iZyGjoy=s64","userId":"12751544315343024734"}},"outputId":"b943d10b-b075-47b6-ed0e-1c0e507bed7f"},"source":["get_ipython().system_raw('./ngrok http 8501 &')\n","! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n","    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""],"execution_count":8,"outputs":[{"output_type":"stream","text":["http://2a5232c07eda.ngrok.io\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7p-un9t8SSxP","outputId":"4deec55d-243d-48fc-f016-e59a5eefeb0d"},"source":["!streamlit run web.py"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[0m\n","\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n","\u001b[0m\n","\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.2:8501\u001b[0m\n","\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://35.232.135.251:8501\u001b[0m\n","\u001b[0m\n","2020-12-04 13:29:42.930 Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","2020-12-04 13:29:43.304269: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n","2020-12-04 13:29:46.613 Lock 140263853967288 acquired on /root/.cache/torch/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.637c6035640bacb831febcc2b7f7bee0a96f9b30c2d7e9ef84082d9f252f3170.lock\n","Downloading: 100% 433/433 [00:00<00:00, 415kB/s]\n","2020-12-04 13:29:46.941 Lock 140263853967288 released on /root/.cache/torch/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.637c6035640bacb831febcc2b7f7bee0a96f9b30c2d7e9ef84082d9f252f3170.lock\n","2020-12-04 13:29:47.047 Lock 140262846307688 acquired on /root/.cache/torch/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.lock\n","Downloading: 100% 232k/232k [00:00<00:00, 1.77MB/s]\n","2020-12-04 13:29:47.296 Lock 140262846307688 released on /root/.cache/torch/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.lock\n","2020-12-04 13:29:49.147 https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache, downloading to /tmp/tmpibp8kflp\n","100% 231508/231508 [00:00<00:00, 1703627.62B/s]\n","2020-12-04 13:29:49.499 copying /tmp/tmpibp8kflp to cache at /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n","2020-12-04 13:29:49.500 creating metadata file for /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n","2020-12-04 13:29:49.501 removing temp file /tmp/tmpibp8kflp\n","2020-12-04 13:29:49.501 loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","2020-12-04 13:42:42.966 loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","2020-12-04 13:42:54.755 loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","2020-12-04 13:42:57.629 loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","2020-12-04 13:49:42.664 loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n","2020-12-04 13:49:42.872 NumExpr defaulting to 2 threads.\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","2020-12-04 13:56:01.868 loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","2020-12-04 13:56:48.538 loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","2020-12-04 13:58:05.095 loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"],"name":"stdout"}]}]}